{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# for tensorflow\r\n",
    "import tensorflow as tf\r\n",
    "import tensorflow.keras.backend as K\r\n",
    "from tensorflow import keras\r\n",
    "\r\n",
    "# other usefull library\r\n",
    "import numpy as np\r\n",
    "import os\r\n",
    "from os.path import join as pjoin\r\n",
    "from datetime import datetime\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "import collections\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "path_drive = \"gdrive/My Drive/ColabNotebooks/\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def split_train_val(args, per_val=0.1):\r\n",
    "    \"\"\"\r\n",
    "    Separa a base em treino e validação\r\n",
    "    :parâmetro args: argumentos \r\n",
    "    :parâmetro per_val: representar a proporção do conjunto de dados a ser incluído na divisão de validação (entre 0,0 e 1,0)\r\n",
    "    \"\"\"\r\n",
    "    # create inline and crossline sections for training and validation:\r\n",
    "    loader_type = 'section'\r\n",
    "    labels = np.load(pjoin((path_drive + 'data'), 'train', 'train_labels.npy'))\r\n",
    "    \r\n",
    "    i_list = list(range(labels.shape[0]))\r\n",
    "    i_list = ['i_'+str(inline) for inline in i_list]\r\n",
    "    \r\n",
    "    x_list = list(range(labels.shape[1]))\r\n",
    "    x_list = ['x_'+str(crossline) for crossline in x_list]\r\n",
    "    \r\n",
    "    list_train_val = i_list + x_list\r\n",
    "    \r\n",
    "    # create train and test splits:\r\n",
    "    list_train, list_val = train_test_split(\r\n",
    "        list_train_val, test_size=per_val, shuffle=True)\r\n",
    "\r\n",
    "    #write to files to disK:\r\n",
    "    file_object = open(\r\n",
    "        pjoin((path_drive + 'data'), 'splits', loader_type + '_train_val.txt'), 'w')\r\n",
    "    file_object.write('\\n'.join(list_train_val))\r\n",
    "    file_object.close()\r\n",
    "    file_object = open(\r\n",
    "        pjoin((path_drive + 'data'), 'splits', loader_type + '_train.txt'), 'w')\r\n",
    "    file_object.write('\\n'.join(list_train))\r\n",
    "    file_object.close()\r\n",
    "    file_object = open(pjoin((path_drive + 'data'), 'splits', loader_type + '_val.txt'), 'w')\r\n",
    "    file_object.write('\\n'.join(list_val))\r\n",
    "    file_object.close()\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class section_loader():\r\n",
    "  \"\"\"\r\n",
    "      Data loader for the section-based deconvnet\r\n",
    "  \"\"\"\r\n",
    "  def __init__(self, split='train', is_transform=True,\r\n",
    "                augmentations=None):      \r\n",
    "    \"\"\"\r\n",
    "    Inicializa os parâmetros da classe\r\n",
    "    :parâmetro split: argumentos \r\n",
    "    :parâmetro is_transform: argumentos \r\n",
    "    :parâmetro augmentations:  \r\n",
    "    \"\"\"      \r\n",
    "    data = path_drive + 'data/' #path\r\n",
    "    self.root = data\r\n",
    "    self.split = split\r\n",
    "    self.is_transform = is_transform\r\n",
    "    self.augmentations = augmentations\r\n",
    "    self.n_classes = 6 \r\n",
    "    self.mean = 0.000941 # average of the training data  \r\n",
    "    self.sections = collections.defaultdict(list)\r\n",
    "\r\n",
    "    if 'test' not in self.split: \r\n",
    "      # Normal train/val mode\r\n",
    "      self.seismic = np.load(pjoin((path_drive + 'data'),'train','train_seismic.npy'))\r\n",
    "      self.labels = np.load(pjoin((path_drive + 'data'),'train','train_labels.npy'))\r\n",
    "    elif 'test1' in self.split:\r\n",
    "      self.seismic = np.load(pjoin((path_drive + 'data'),'test_once','test1_seismic.npy'))\r\n",
    "      self.labels = np.load(pjoin((path_drive + 'data'),'test_once','test1_labels.npy'))\r\n",
    "    elif 'test2' in self.split:\r\n",
    "      self.seismic = np.load(pjoin((path_drive + 'data'),'test_once','test2_seismic.npy'))\r\n",
    "      self.labels = np.load(pjoin((path_drive + 'data'),'test_once','test2_labels.npy'))\r\n",
    "    else:\r\n",
    "      raise ValueError('Unknown split.')\r\n",
    "\r\n",
    "    if 'test' not in self.split:\r\n",
    "      # We are in train/val mode. Most likely the test splits are not saved yet, \r\n",
    "      # so don't attempt to load them.  \r\n",
    "      for split in ['train', 'val', 'train_val']:\r\n",
    "        # reading the file names for 'train', 'val', 'trainval'\"\"\r\n",
    "        path = pjoin((path_drive + 'data'), 'splits', 'section_' + split + '.txt')\r\n",
    "        file_list = tuple(open(path, 'r'))\r\n",
    "        file_list = [id_.rstrip() for id_ in file_list]\r\n",
    "        self.sections[split] = file_list\r\n",
    "    elif 'test' in split:\r\n",
    "      # We are in test mode. Only read the given split. The other one might not \r\n",
    "      # be available. \r\n",
    "      path = pjoin((path_drive + 'data'), 'splits', 'section_' + split + '.txt')\r\n",
    "      file_list = tuple(open(path,'r'))\r\n",
    "      file_list = [id_.rstrip() for id_ in file_list]\r\n",
    "      self.sections[split] = file_list\r\n",
    "    else:\r\n",
    "      raise ValueError('Unknown split.')\r\n",
    "\r\n",
    "\r\n",
    "  def __len__(self):\r\n",
    "    return len(self.sections[self.split])\r\n",
    "\r\n",
    "  def __getitem__(self, index):\r\n",
    "\r\n",
    "    section_name = self.sections[self.split][index]\r\n",
    "    direction, number = section_name.split(sep='_')\r\n",
    "\r\n",
    "    if direction == 'i':\r\n",
    "        im = self.seismic[int(number),:,:]\r\n",
    "        lbl = self.labels[int(number),:,:]\r\n",
    "    elif direction == 'x':    \r\n",
    "        im = self.seismic[:,int(number),:]\r\n",
    "        lbl = self.labels[:,int(number),:]\r\n",
    "    \r\n",
    "    if self.augmentations is not None:\r\n",
    "        im, lbl = self.augmentations(im, lbl)\r\n",
    "        \r\n",
    "    if self.is_transform:\r\n",
    "        im, lbl = self.transform(im, lbl)\r\n",
    "    return im, lbl\r\n",
    "\r\n",
    "\r\n",
    "  def transform(self, img, lbl):\r\n",
    "    \"\"\"\r\n",
    "    Transforma os dados na posição correta e normaliza\r\n",
    "    :parâmetro img: dados da imagem  \r\n",
    "    :parâmetro lbl: dados das labels \r\n",
    "    return: img(imagem), lbl(label)\r\n",
    "    \"\"\" \r\n",
    "    img -= self.mean\r\n",
    "\r\n",
    "    # to be in the BxCxHxW: \r\n",
    "    img, lbl = img.T, lbl.T\r\n",
    "\r\n",
    "    img = np.array(img)\r\n",
    "    lbl = np.array(lbl)\r\n",
    "\r\n",
    "    #normalização 0-1 (os dados estão entre 1 a -1)\r\n",
    "    scaler = MinMaxScaler(feature_range=(0,1))\r\n",
    "    scaler = scaler.fit(img)\r\n",
    "    img = scaler.fit_transform(img)\r\n",
    "            \r\n",
    "    return img, lbl"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.11 64-bit ('mestrado': conda)"
  },
  "interpreter": {
   "hash": "447cc6f42bbc2aea3361993c39d6f6c3959bdf1889010beb1442d5334007d2b1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}