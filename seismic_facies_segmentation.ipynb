{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# for tensorflow\r\n",
    "import tensorflow as tf\r\n",
    "import tensorflow.keras.backend as K\r\n",
    "from tensorflow import keras\r\n",
    "\r\n",
    "# other usefull library\r\n",
    "import numpy as np\r\n",
    "import os\r\n",
    "from os.path import join as pjoin\r\n",
    "from datetime import datetime\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "import collections\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "path_drive = \"gdrive/My Drive/ColabNotebooks/\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def split_train_val(args, per_val=0.1):\r\n",
    "    \"\"\"\r\n",
    "    Separa a base em treino e validação\r\n",
    "    :parâmetro args: argumentos \r\n",
    "    :parâmetro per_val: representar a proporção do conjunto de dados a ser incluído na divisão de validação (entre 0,0 e 1,0)\r\n",
    "    \"\"\"\r\n",
    "    # create inline and crossline sections for training and validation:\r\n",
    "    loader_type = 'section'\r\n",
    "    labels = np.load(pjoin((path_drive + 'data'), 'train', 'train_labels.npy'))\r\n",
    "    \r\n",
    "    i_list = list(range(labels.shape[0]))\r\n",
    "    i_list = ['i_'+str(inline) for inline in i_list]\r\n",
    "    \r\n",
    "    x_list = list(range(labels.shape[1]))\r\n",
    "    x_list = ['x_'+str(crossline) for crossline in x_list]\r\n",
    "    \r\n",
    "    list_train_val = i_list + x_list\r\n",
    "    \r\n",
    "    # create train and test splits:\r\n",
    "    list_train, list_val = train_test_split(\r\n",
    "        list_train_val, test_size=per_val, shuffle=True)\r\n",
    "\r\n",
    "    #write to files to disK:\r\n",
    "    file_object = open(pjoin((path_drive + 'data'), 'splits', loader_type + '_train_val.txt'), 'w')\r\n",
    "    file_object.write('\\n'.join(list_train_val))\r\n",
    "    file_object.close()\r\n",
    "    file_object = open(pjoin((path_drive + 'data'), 'splits', loader_type + '_train.txt'), 'w')\r\n",
    "    file_object.write('\\n'.join(list_train))\r\n",
    "    file_object.close()\r\n",
    "    file_object = open(pjoin((path_drive + 'data'), 'splits', loader_type + '_val.txt'), 'w')\r\n",
    "    file_object.write('\\n'.join(list_val))\r\n",
    "    file_object.close()\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class section_loader():\r\n",
    "\t\"\"\"\r\n",
    "\tData loader for the section-based deconvnet\r\n",
    "\t\"\"\"\r\n",
    "\tdef __init__(self, split='train', is_transform=True, augmentations=None):      \r\n",
    "\t\t\"\"\"\r\n",
    "\t\tInicializa os parâmetros da classe\r\n",
    "\t\t:parâmetro split: argumentos \r\n",
    "\t\t:parâmetro is_transform: argumentos \r\n",
    "\t\t:parâmetro augmentations:  \r\n",
    "\t\t\"\"\"      \r\n",
    "\t\tdata = path_drive + 'data/' #path\r\n",
    "\t\tself.root = data\r\n",
    "\t\tself.split = split\r\n",
    "\t\tself.is_transform = is_transform\r\n",
    "\t\tself.augmentations = augmentations\r\n",
    "\t\tself.n_classes = 6 \r\n",
    "\t\tself.mean = 0.000941 # average of the training data  \r\n",
    "\t\tself.sections = collections.defaultdict(list)\r\n",
    "\r\n",
    "\t\tif 'test' not in self.split: \r\n",
    "\t\t\t# Normal train/val mode\r\n",
    "\t\t\tself.seismic = np.load(pjoin((path_drive + 'data'),'train','train_seismic.npy'))\r\n",
    "\t\t\tself.labels = np.load(pjoin((path_drive + 'data'),'train','train_labels.npy'))\r\n",
    "\t\telif 'test1' in self.split:\r\n",
    "\t\t\tself.seismic = np.load(pjoin((path_drive + 'data'),'test_once','test1_seismic.npy'))\r\n",
    "\t\t\tself.labels = np.load(pjoin((path_drive + 'data'),'test_once','test1_labels.npy'))\r\n",
    "\t\telif 'test2' in self.split:\r\n",
    "\t\t\tself.seismic = np.load(pjoin((path_drive + 'data'),'test_once','test2_seismic.npy'))\r\n",
    "\t\t\tself.labels = np.load(pjoin((path_drive + 'data'),'test_once','test2_labels.npy'))\r\n",
    "\t\telse:\r\n",
    "\t\t\traise ValueError('Unknown split.')\r\n",
    "\r\n",
    "\t\tif 'test' not in self.split:\r\n",
    "\t\t\t# We are in train/val mode. Most likely the test splits are not saved yet, \r\n",
    "\t\t\t# so don't attempt to load them.  \r\n",
    "\t\t\tfor split in ['train', 'val', 'train_val']:\r\n",
    "\t\t\t\t# reading the file names for 'train', 'val', 'trainval'\"\"\r\n",
    "\t\t\t\tpath = pjoin((path_drive + 'data'), 'splits', 'section_' + split + '.txt')\r\n",
    "\t\t\t\tfile_list = tuple(open(path, 'r'))\r\n",
    "\t\t\t\tfile_list = [id_.rstrip() for id_ in file_list]\r\n",
    "\t\t\t\tself.sections[split] = file_list\r\n",
    "\t\telif 'test' in split:\r\n",
    "\t\t\t# We are in test mode. Only read the given split. The other one might not \r\n",
    "\t\t\t# be available. \r\n",
    "\t\t\tpath = pjoin((path_drive + 'data'), 'splits', 'section_' + split + '.txt')\r\n",
    "\t\t\tfile_list = tuple(open(path,'r'))\r\n",
    "\t\t\tfile_list = [id_.rstrip() for id_ in file_list]\r\n",
    "\t\t\tself.sections[split] = file_list\r\n",
    "\t\telse:\r\n",
    "\t\t\traise ValueError('Unknown split.')\r\n",
    "\r\n",
    "\r\n",
    "\tdef __len__(self):\r\n",
    "\t\treturn len(self.sections[self.split])\r\n",
    "\r\n",
    "\tdef __getitem__(self, index):\r\n",
    "\r\n",
    "\t\tsection_name = self.sections[self.split][index]\r\n",
    "\t\tdirection, number = section_name.split(sep='_')\r\n",
    "\r\n",
    "\t\tif direction == 'i':\r\n",
    "\t\t\t\tim = self.seismic[int(number),:,:]\r\n",
    "\t\t\t\tlbl = self.labels[int(number),:,:]\r\n",
    "\t\telif direction == 'x':    \r\n",
    "\t\t\t\tim = self.seismic[:,int(number),:]\r\n",
    "\t\t\t\tlbl = self.labels[:,int(number),:]\r\n",
    "\t\t\r\n",
    "\t\tif self.augmentations is not None:\r\n",
    "\t\t\t\tim, lbl = self.augmentations(im, lbl)\r\n",
    "\t\t\t\t\r\n",
    "\t\tif self.is_transform:\r\n",
    "\t\t\t\tim, lbl = self.transform(im, lbl)\r\n",
    "\t\treturn im, lbl\r\n",
    "\r\n",
    "\r\n",
    "\tdef transform(self, img, lbl):\r\n",
    "\t\t\"\"\"\r\n",
    "\t\tTransforma os dados na posição correta e normaliza\r\n",
    "\t\t:parâmetro img: dados da imagem  \r\n",
    "\t\t:parâmetro lbl: dados das labels \r\n",
    "\t\treturn: img(imagem), lbl(label)\r\n",
    "\t\t\"\"\" \r\n",
    "\t\timg -= self.mean\r\n",
    "\r\n",
    "\t\t# to be in the BxCxHxW: \r\n",
    "\t\timg, lbl = img.T, lbl.T\r\n",
    "\r\n",
    "\t\timg = np.array(img)\r\n",
    "\t\tlbl = np.array(lbl)\r\n",
    "\r\n",
    "\t\t#normalização 0-1 (os dados estão entre 1 a -1)\r\n",
    "\t\tscaler = MinMaxScaler(feature_range=(0,1))\r\n",
    "\t\tscaler = scaler.fit(img)\r\n",
    "\t\timg = scaler.fit_transform(img)\r\n",
    "\t\t\t\t\t\t\r\n",
    "\t\treturn img, lbl"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def processWeights(image_label):\r\n",
    "    \"\"\"\r\n",
    "    Contabiliza os valores dos pixels por classe para definir o peso\r\n",
    "    :parâmetro image_label: imagem com as labels \r\n",
    "    return: weights (pesos de cada classe na imagem)\r\n",
    "    \"\"\" \r\n",
    "    image_ = image_label\r\n",
    "    (unique, counts) = np.unique(image_, return_counts=True)\r\n",
    "    #weights for each class\r\n",
    "    num_pixels = image_.shape[0] * image_.shape[1] \r\n",
    "    weights_ = [0,0,0,0,0,0]\r\n",
    "    for i in range(len(counts)):\r\n",
    "        weights_[i] = counts[i]/num_pixels\r\n",
    "    \r\n",
    "    return weights_"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def resizePotencia2(image, size, flag):\r\n",
    "\t\"\"\"\r\n",
    "\tRedefine o tamanho da imagem\r\n",
    "\t:parâmetro image: imagem\r\n",
    "\t:parâmetro size: \r\n",
    "\t:parâmetro flag: tipo de redefinição\r\n",
    "\t\t\t\t\t- 'preencher': preenche com valores próximos\r\n",
    "\t\t\t\t\t- 'rezize': função do tensorflow que redefine a imagem\r\n",
    "\t\t\t\t\t- '': Copia a imagem para uma imagem quadrada adicionando a classe 0 de background\r\n",
    "\treturn: matriz \r\n",
    "\t\"\"\" \r\n",
    "\tif flag == 'resize':\r\n",
    "\t\treturn tf.image.resize(image, [size,size])\r\n",
    "\r\n",
    "\timage_ = image\r\n",
    "\tresult = np.zeros((size, size))\r\n",
    "\tresult[:image_.shape[0],:image_.shape[1]] = image_[:,:]\r\n",
    "\r\n",
    "\tif flag == 'preencher':\r\n",
    "\t\tfor i in range(0, size):\r\n",
    "\t\t\tfor j in range(image_.shape[1]-1, size):\r\n",
    "\t\t\t\tresult[i][j] = result[i][j-1]\r\n",
    "\r\n",
    "\t\tfor i in range(image_.shape[0]-1, size):\r\n",
    "\t\t\tfor j in range(0, size):\r\n",
    "\t\t\t\tresult[i][j] = result[i-1][j]\r\n",
    "\r\n",
    "\treturn result"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def buildingSet(examples_set):\r\n",
    "    # construção de um conjunto, vetor de imagens e labels e seus pesos\r\n",
    "    samples = []\r\n",
    "    samples_labels = []\r\n",
    "    count = 0\r\n",
    "    for i, (imagens, labels) in enumerate(examples_set):\r\n",
    "        image_original, labels_original = imagens, labels\r\n",
    "\r\n",
    "        # Contabiliza a proporção de pixels de casa classe\r\n",
    "        weights_ = processWeights(labels_original)\r\n",
    "        for j in range(len(weights_class)):                \r\n",
    "            weights_class[j] = weights_class[j] + weights_[j] \r\n",
    "    \r\n",
    "        count = count + 1 \r\n",
    "\r\n",
    "        # image_original = resizePotencia2(image_original, 512, 'resize')\r\n",
    "        # labels_original = resizePotencia2(labels_original, 512, 'resize')\r\n",
    "\r\n",
    "        image_original = np.expand_dims(image_original, axis=-1) \r\n",
    "        labels_original = np.expand_dims(labels_original, axis=-1)\r\n",
    "\r\n",
    "        image_original = tf.image.resize(image_original, [256,256])\r\n",
    "        labels_original = tf.image.resize(labels_original, [256,256])  \r\n",
    "\r\n",
    "        samples.append(image_original)\r\n",
    "        samples_labels.append(labels_original)\r\n",
    "\r\n",
    "    for j in range(len(weights_class)):                \r\n",
    "        weights_class[j] = weights_class[j]/count\r\n",
    "\r\n",
    "    return np.array(samples), np.array(samples_labels), np.array(weights_class)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def tfDataset(dataset_samples, dataset_labels, num_classes, SHUFFLE_BUFFER_SIZE, BATCH_SIZE):\r\n",
    "\r\n",
    "    # Divide a imagem da label em 6 referente as classes (uma máscara para cada classe)\r\n",
    "    dataset_labels = tf.keras.utils.to_categorical(dataset_labels, num_classes=num_classes)\r\n",
    "    #Tensor com o conjunto de dados X e Y\r\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((dataset_samples, dataset_labels))\r\n",
    "    #mantém um buffer size elementos e seleciona aleatoriamente o próximo elemento desse buffer\r\n",
    "    dataset = dataset.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)\r\n",
    "\r\n",
    "    return dataset"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Fluxo da construção dos dados de treinamento"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Argumentos\r\n",
    "path_drive = \"gdrive/My Drive/ColabNotebooks/\"\r\n",
    "BATCH_SIZE = 8\r\n",
    "SHUFFLE_BUFFER_SIZE = 16\r\n",
    "num_classes = 6\r\n",
    "weights_class = [0,0,0,0,0,0]\r\n",
    "\r\n",
    "args = {\r\n",
    "              \"arch\": 'section_deconvnet',\r\n",
    "              \"n_epoch\": 61,\r\n",
    "              \"batch_size\": 8,\r\n",
    "              \"resume\": None,\r\n",
    "              \"clip\": 0.1,\r\n",
    "              \"per_val\": 0.1,\r\n",
    "              \"pretrained\": False,\r\n",
    "              \"aug\": True,\r\n",
    "              \"class_weights\": False,\r\n",
    "            }\r\n",
    "\r\n",
    "\"\"\"1. Load the dataset: Generate the train and validation sets for the model\"\"\"\r\n",
    "#split_train_val(args, per_val=args['per_val'])\r\n",
    "\r\n",
    "\"\"\"2. Load the dataset: object set \"\"\"\r\n",
    "#object to train\r\n",
    "train_set = section_loader(is_transform=True, split='train',)    \r\n",
    "    \r\n",
    "# object to validation\r\n",
    "val_set = section_loader(is_transform=True, split='val',)\r\n",
    "\r\n",
    "\"\"\"3. Dataset: building Set \"\"\"\r\n",
    "#construção do vetor de imagens de treino\r\n",
    "train_samples, train_labels, weights_class = buildingSet(train_set)\r\n",
    "train_samples, train_labels = train_samples[0:200,:,:], train_labels[0:200,:,:]\r\n",
    "\r\n",
    "#construção do vetor de imagens de validação\r\n",
    "val_samples, val_labels, weights_class_validation = buildingSet(val_set)\r\n",
    "val_samples, val_labels = val_samples[0:20,:,:], val_labels[0:20,:,:]\r\n",
    "\r\n",
    "\"\"\"4. Dataset: tf dataset \"\"\"\r\n",
    "# train\r\n",
    "train_dataset = tfDataset(train_samples, train_labels, num_classes, SHUFFLE_BUFFER_SIZE, BATCH_SIZE)\r\n",
    "# Validation\r\n",
    "validation_dataset = tfDataset(val_samples, val_labels, num_classes, SHUFFLE_BUFFER_SIZE, BATCH_SIZE)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Visualizar informações dos dados de treino\r\n",
    "print(\"Dados de treino\")\r\n",
    "print(\"Samples e labels: \", train_samples.shape, train_labels.shape)\r\n",
    "print(\"Dados de validação\")\r\n",
    "print(\"Samples e labels: \",  val_samples.shape, val_labels.shape)\r\n",
    "print(\"Pesos: \" , weights_class)\r\n",
    "\r\n",
    "fig, ax = plt.subplots(1,2)\r\n",
    "ax[0].imshow(np.array(train_samples[0][:,:,0]))\r\n",
    "ax[1].imshow(np.array(train_labels[0][:,:,0]))\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Visualizar informações dos dados TF de treino\r\n",
    "print(\"TF dataset of train: \", train_dataset)\r\n",
    "print(\"TF dataset of validation: \", validation_dataset)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# deletar dados que não serão mais usados: objetos com os conjuntos dos dados\r\n",
    "del train_samples, train_labels, val_samples, val_labels, weights_class_validation"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Rede Neural"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Encoder / Decoder"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def unet_model(input_shape, n_classes):\r\n",
    "\t\"\"\" Define o conjuntode treino e validação\r\n",
    "\r\n",
    "\tParameters:\r\n",
    "\t\tinput_shape ( ): Shape de entrada (imagens)\r\n",
    "\t\tn_classes (Integer): Quantidade de classes\r\n",
    "\r\n",
    "\tReturns: \r\n",
    "\t\tObject: Modelo da rede\r\n",
    "\t\"\"\"\r\n",
    "\r\n",
    "\tinputs = tf.keras.layers.Input(input_shape)\r\n",
    "\tx = inputs\r\n",
    "\r\n",
    "\t# ------------------- Encoder ------------------- #\r\n",
    "\r\n",
    "\t# Block encoder 1\r\n",
    "\tconv_enc_1 = tf.keras.layers.Conv2D(64, kernel_size=3, strides=1, padding=\"same\", activation= \"relu\")(x)\r\n",
    "\tconv_enc_1 = tf.keras.layers.BatchNormalization(momentum=0.1,epsilon=1e-05)(conv_enc_1)\r\n",
    "\tconv_enc_1 = tf.keras.layers.Conv2D(64, kernel_size=3, strides=1, padding=\"same\", activation= \"relu\")(conv_enc_1)\r\n",
    "\tconv_enc_1 = tf.keras.layers.BatchNormalization(momentum=0.1,epsilon=1e-05)(conv_enc_1)\r\n",
    "\tmax_pool_enc_1 = tf.keras.layers.MaxPool2D(pool_size=(2,2))(conv_enc_1)\r\n",
    "\r\n",
    "\t# Block encoder 2\r\n",
    "\tconv_enc_2 = tf.keras.layers.Conv2D(128, kernel_size=3, strides=1, padding=\"same\", activation= \"relu\")(max_pool_enc_1)\r\n",
    "\tconv_enc_2 = tf.keras.layers.BatchNormalization(momentum=0.1,epsilon=1e-05)(conv_enc_2)\r\n",
    "\tconv_enc_2 = tf.keras.layers.Conv2D(128, kernel_size=3, strides=1, padding=\"same\", activation= \"relu\")(conv_enc_2)\r\n",
    "\tconv_enc_2 = tf.keras.layers.BatchNormalization(momentum=0.1,epsilon=1e-05)(conv_enc_2)\r\n",
    "\tmax_pool_enc_2 = tf.keras.layers.MaxPool2D(pool_size=(2,2))(conv_enc_2)\r\n",
    "\r\n",
    "\t# Block  encoder 3\r\n",
    "\tconv_enc_3 = tf.keras.layers.Conv2D(256, kernel_size=3, strides=1, padding=\"same\", activation= \"relu\")(max_pool_enc_2)\r\n",
    "\tconv_enc_3 = tf.keras.layers.BatchNormalization(momentum=0.1,epsilon=1e-05)(conv_enc_3)\r\n",
    "\tconv_enc_3 = tf.keras.layers.Conv2D(256, kernel_size=3, strides=1, padding=\"same\", activation= \"relu\")(conv_enc_3)\r\n",
    "\tconv_enc_3 = tf.keras.layers.BatchNormalization(momentum=0.1,epsilon=1e-05)(conv_enc_3)\r\n",
    "\tconv_enc_3 = tf.keras.layers.Conv2D(256, kernel_size=3, strides=1, padding=\"same\", activation= \"relu\")(conv_enc_3)\r\n",
    "\tconv_enc_3 = tf.keras.layers.BatchNormalization(momentum=0.1,epsilon=1e-05)(conv_enc_3)\r\n",
    "\tmax_pool_enc_3 = tf.keras.layers.MaxPool2D(pool_size=(2,2))(conv_enc_3)\r\n",
    "\r\n",
    "\t# Block  encoder 4\r\n",
    "\tconv_enc_4 = tf.keras.layers.Conv2D(512, kernel_size=3, strides=1, padding=\"same\", activation= \"relu\")(max_pool_enc_3)\r\n",
    "\tconv_enc_4 = tf.keras.layers.BatchNormalization(momentum=0.1,epsilon=1e-05)(conv_enc_4)\r\n",
    "\tconv_enc_4 = tf.keras.layers.Conv2D(512, kernel_size=3, strides=1, padding=\"same\", activation= \"relu\")(conv_enc_4)\r\n",
    "\tconv_enc_4 = tf.keras.layers.BatchNormalization(momentum=0.1,epsilon=1e-05)(conv_enc_4)\r\n",
    "\tconv_enc_4 = tf.keras.layers.Conv2D(512, kernel_size=3, strides=1, padding=\"same\", activation= \"relu\")(conv_enc_4)\r\n",
    "\tmax_pool_enc_4 = tf.keras.layers.MaxPool2D(pool_size=(2,2))(conv_enc_4)\r\n",
    "\r\n",
    "\t# Block  encoder 5\r\n",
    "\tconv_enc_5 = tf.keras.layers.Conv2D(512, kernel_size=3, strides=1, padding=\"same\", activation= \"relu\")(max_pool_enc_4)\r\n",
    "\tconv_enc_5 = tf.keras.layers.BatchNormalization(momentum=0.1,epsilon=1e-05)(conv_enc_5)\r\n",
    "\tconv_enc_5 = tf.keras.layers.Conv2D(512, kernel_size=3, strides=1, padding=\"same\", activation= \"relu\")(conv_enc_5)\r\n",
    "\tconv_enc_5 = tf.keras.layers.BatchNormalization(momentum=0.1,epsilon=1e-05)(conv_enc_5)\r\n",
    "\tconv_enc_5 = tf.keras.layers.Conv2D(512, kernel_size=3, strides=1, padding=\"same\", activation= \"relu\")(conv_enc_5)\r\n",
    "\tmax_pool_enc_5 = tf.keras.layers.MaxPool2D(pool_size=(2,2))(conv_enc_5)\r\n",
    "\r\n",
    "\t# Block  encoder 6\r\n",
    "\tconv_enc_6 = tf.keras.layers.Conv2D(4096, kernel_size=3, strides=1, padding=\"same\", activation= \"relu\")(max_pool_enc_5)\r\n",
    "\tconv_enc_6 = tf.keras.layers.BatchNormalization(momentum=0.1,epsilon=1e-05)(conv_enc_6)\r\n",
    "\r\n",
    "\t# Block  encoder 7\r\n",
    "\tconv_enc_7 = tf.keras.layers.Conv2D(4096, kernel_size=3, strides=1, padding=\"same\", activation= \"relu\")(conv_enc_6)\r\n",
    "\tconv_enc_7 = tf.keras.layers.BatchNormalization(momentum=0.1,epsilon=1e-05)(conv_enc_7)\r\n",
    "\t\r\n",
    "\t# ------------------- Decoder ------------------- #\r\n",
    "\r\n",
    "\t# Block  decoder 8\r\n",
    "\tconv_dec_8 = tf.keras.layers.Conv2DTranspose(4096, kernel_size=3, strides=1, padding=\"same\", activation= \"relu\")(conv_enc_7)\r\n",
    "\tconv_dec_8 = tf.keras.layers.BatchNormalization(momentum=0.1,epsilon=1e-05)(conv_dec_8)\r\n",
    "\r\n",
    "\t# Block  decoder 9\r\n",
    "\tup_dec_9 = tf.keras.layers.UpSampling2D(2)(conv_dec_8)\r\n",
    "\tup_dec_9 = tf.keras.layers.Concatenate()([up_dec_9, conv_enc_5])\r\n",
    "\r\n",
    "\t# Block  decoder 10\r\n",
    "\tconv_dec_10 = tf.keras.layers.Conv2DTranspose(512, kernel_size=3, strides=1, padding=\"same\", activation= \"relu\")(up_dec_9)\r\n",
    "\tconv_dec_10 = tf.keras.layers.BatchNormalization(momentum=0.1,epsilon=1e-05)(conv_dec_10)\r\n",
    "\tconv_dec_10 = tf.keras.layers.Conv2DTranspose(512, kernel_size=3, strides=1, padding=\"same\", activation= \"relu\")(conv_dec_10)\r\n",
    "\tconv_dec_10 = tf.keras.layers.BatchNormalization(momentum=0.1,epsilon=1e-05)(conv_dec_10)\r\n",
    "\tconv_dec_10 = tf.keras.layers.Conv2DTranspose(512, kernel_size=3, strides=1, padding=\"same\", activation= \"relu\")(conv_dec_10)\r\n",
    "\tconv_dec_10 = tf.keras.layers.BatchNormalization(momentum=0.1,epsilon=1e-05)(conv_dec_10)\r\n",
    "\r\n",
    "\t# Block  decoder 11\r\n",
    "\tup_dec_11 = tf.keras.layers.UpSampling2D(2)(conv_dec_10)\r\n",
    "\tup_dec_11 = tf.keras.layers.Concatenate()([up_dec_11, conv_enc_4])\r\n",
    "\r\n",
    "\t# Block  decoder 12\r\n",
    "\tconv_dec_12 = tf.keras.layers.Conv2DTranspose(512, kernel_size=3, strides=1, padding=\"same\", activation= \"relu\")(up_dec_11)\r\n",
    "\tconv_dec_12 = tf.keras.layers.BatchNormalization(momentum=0.1,epsilon=1e-05)(conv_dec_12)\r\n",
    "\tconv_dec_12 = tf.keras.layers.Conv2DTranspose(512, kernel_size=3, strides=1, padding=\"same\", activation= \"relu\")(conv_dec_12)\r\n",
    "\tconv_dec_12 = tf.keras.layers.BatchNormalization(momentum=0.1,epsilon=1e-05)(conv_dec_12)\r\n",
    "\tconv_dec_12 = tf.keras.layers.Conv2DTranspose(256, kernel_size=3, strides=1, padding=\"same\", activation= \"relu\")(conv_dec_12)\r\n",
    "\tconv_dec_12 = tf.keras.layers.BatchNormalization(momentum=0.1,epsilon=1e-05)(conv_dec_12)\r\n",
    "\r\n",
    "\t# Block  decoder 13\r\n",
    "\tup_dec_13 = tf.keras.layers.UpSampling2D(2)(conv_dec_12)\r\n",
    "\tup_dec_13 = tf.keras.layers.Concatenate()([up_dec_13, conv_enc_3])\r\n",
    "\r\n",
    "\t# Block  decoder 14\r\n",
    "\tconv_dec_14 = tf.keras.layers.Conv2DTranspose(256, kernel_size=3, strides=1, padding=\"same\", activation= \"relu\")(up_dec_13)\r\n",
    "\tconv_dec_14 = tf.keras.layers.BatchNormalization(momentum=0.1,epsilon=1e-05)(conv_dec_14)\r\n",
    "\tconv_dec_14 = tf.keras.layers.Conv2DTranspose(256, kernel_size=3, strides=1, padding=\"same\", activation= \"relu\")(conv_dec_14)\r\n",
    "\tconv_dec_14 = tf.keras.layers.BatchNormalization(momentum=0.1,epsilon=1e-05)(conv_dec_14)\r\n",
    "\tconv_dec_14 = tf.keras.layers.Conv2DTranspose(128, kernel_size=3, strides=1, padding=\"same\", activation= \"relu\")(conv_dec_14)\r\n",
    "\tconv_dec_14 = tf.keras.layers.BatchNormalization(momentum=0.1,epsilon=1e-05)(conv_dec_14)\r\n",
    "\r\n",
    "\t# Block  decoder 15\r\n",
    "\tup_dec_15 = tf.keras.layers.UpSampling2D(2)(conv_dec_14)\r\n",
    "\tup_dec_15 = tf.keras.layers.Concatenate()([up_dec_15, conv_enc_2])\r\n",
    "\r\n",
    "\t# Block  decoder 16\r\n",
    "\tconv_dec_16 = tf.keras.layers.Conv2DTranspose(128, kernel_size=3, strides=1, padding=\"same\", activation= \"relu\")(up_dec_15)\r\n",
    "\tconv_dec_16 = tf.keras.layers.BatchNormalization(momentum=0.1,epsilon=1e-05)(conv_dec_16)\r\n",
    "\tconv_dec_16 = tf.keras.layers.Conv2DTranspose(128, kernel_size=3, strides=1, padding=\"same\", activation= \"relu\")(conv_dec_16)\r\n",
    "\tconv_dec_16 = tf.keras.layers.BatchNormalization(momentum=0.1,epsilon=1e-05)(conv_dec_16)\r\n",
    "\r\n",
    "\t# Block  decoder 17\r\n",
    "\tup_dec_17 = tf.keras.layers.UpSampling2D(2)(conv_dec_16)\r\n",
    "\tup_dec_17 = tf.keras.layers.Concatenate()([up_dec_17, conv_enc_1])\r\n",
    "\r\n",
    "\t# Block  decoder 18\r\n",
    "\tconv_dec_18 = tf.keras.layers.Conv2DTranspose(64, kernel_size=3, strides=1, padding=\"same\", activation= \"relu\")(up_dec_17)\r\n",
    "\tconv_dec_18 = tf.keras.layers.BatchNormalization(momentum=0.1,epsilon=1e-05)(conv_dec_18)\r\n",
    "\tconv_dec_18 = tf.keras.layers.Conv2DTranspose(64, kernel_size=3, strides=1, padding=\"same\", activation= \"relu\")(conv_dec_18)\r\n",
    "\tconv_dec_18 = tf.keras.layers.BatchNormalization(momentum=0.1,epsilon=1e-05)(conv_dec_18)\r\n",
    "\r\n",
    "\t# Block  decoder 19\r\n",
    "\toutput = tf.keras.layers.Conv2D(n_classes, kernel_size=1, padding=\"same\", activation='softmax')(conv_dec_18)  \r\n",
    "\r\n",
    "\tmodel_unet = tf.keras.models.Model(inputs, output)\r\n",
    "\tmodel_unet.summary()\r\n",
    "\r\n",
    "\treturn model_unet"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# cross entropy: com balanceamento de peso\r\n",
    "def weighted_categorical_crossentropy(weights):\r\n",
    "    \"\"\"\r\n",
    "    A weighted version of keras.objectives.categorical_crossentropy\r\n",
    "    \r\n",
    "    Variables:\r\n",
    "        weights: numpy array of shape (C,) where C is the number of classes\r\n",
    "    \r\n",
    "    Usage:\r\n",
    "        weights = np.array([0.5,2,10]) # Class one at 0.5, class 2 twice the normal weights, class 3 10x.\r\n",
    "        loss = weighted_categorical_crossentropy(weights)\r\n",
    "        model.compile(loss=loss,optimizer='adam')\r\n",
    "    \"\"\"\r\n",
    "    weights = K.variable(weights)\r\n",
    "    def loss(y_true, y_pred):\r\n",
    "        # scale predictions so that the class probas of each sample sum to 1\r\n",
    "        y_pred /= K.sum(y_pred, axis=-1, keepdims=True)\r\n",
    "        # clip to prevent NaN's and Inf's\r\n",
    "        y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())\r\n",
    "        loss = y_true * K.log(y_pred) + (1-y_true) * K.log(1-y_pred)\r\n",
    "        loss = loss * weights \r\n",
    "        loss = - K.mean(loss, -1)\r\n",
    "        return loss\r\n",
    "    \r\n",
    "    return loss"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Dice loss\r\n",
    "def diceLoss():  \r\n",
    "    def dice_coef(y_true, y_pred, smooth=1):\r\n",
    "        intersection = keras.backend.sum(y_true * y_pred, axis=[1,2,3])\r\n",
    "        union = keras.backend.sum(y_true, axis=[1,2,3]) + keras.backend.sum(y_pred, axis=[1,2,3])\r\n",
    "        return keras.backend.mean( (2. * intersection + smooth) / (union + smooth), axis=0)\r\n",
    "\r\n",
    "    def loss(y_true, y_pred):\r\n",
    "        loss = -dice_coef(y_true, y_pred)\r\n",
    "        return loss\r\n",
    "    \r\n",
    "    return loss"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Argumentos\r\n",
    "size = 256\r\n",
    "qtd_channel = 1\r\n",
    "\r\n",
    "''' 1 Inicializando modelo '''\r\n",
    "\r\n",
    "input_size =  (size, size, qtd_channel) \r\n",
    "model = unet_model(input_shape=input_size, n_classes=6)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "''' 2 Compilando o modelo '''\r\n",
    "\r\n",
    "# *** 2.1 definindo a função de loss ***\r\n",
    "loss_function = weighted_categorical_crossentropy(np.array(weights_class))\r\n",
    "#loss_function = diceLoss()\r\n",
    "\r\n",
    "# *** 2.2 definindo a callbaks ***\r\n",
    "#Pare o treinamento quando uma métrica monitorada parar de melhorar\r\n",
    "callbacks = [tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=3, mode='min')]\r\n",
    "\r\n",
    "base_learning_rate = 0.0001\r\n",
    "\r\n",
    "# *** 2.3 compilando o modelo ***\r\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(base_learning_rate),\r\n",
    "              loss=loss_function,\r\n",
    "              metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "''' 3 treinando o modelo '''\r\n",
    "\r\n",
    "history = model.fit(train_dataset,epochs=15, validation_data=validation_dataset, callbacks = callbacks)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# deletar dados que não serão mais usados: conjuntos dos dados do tensorflow\r\n",
    "del train_dataset, validation_dataset, loss_function, callbacks"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Salva o modelo\r\n",
    "model.save(path_drive + '/my_model')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Carrega um modelo\r\n",
    "new_model = tf.keras.models.load_model(path_drive + '/my_model')\r\n",
    "\r\n",
    "# Check its architecture\r\n",
    "new_model.summary()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def datasetTest(args):\r\n",
    "\r\n",
    "    splits = [args['split'] if 'both' not in args['split'] else 'test1', 'test2']\r\n",
    "  \r\n",
    "    # construção do vetor de imagens de treino\r\n",
    "    test_ = []\r\n",
    "    label_ = []\r\n",
    "    for sdx, split in enumerate(splits):\r\n",
    "        # define indices of the array\r\n",
    "        labels = np.load(pjoin((path_drive + 'data'), 'test_once', split + '_labels.npy'))\r\n",
    "        irange, xrange, depth = labels.shape\r\n",
    "\r\n",
    "        if args['inline']:\r\n",
    "            i_list = list(range(irange))\r\n",
    "            i_list = ['i_'+str(inline) for inline in i_list]\r\n",
    "        else:\r\n",
    "            i_list = []\r\n",
    "\r\n",
    "        if args['crossline']:\r\n",
    "            x_list = list(range(xrange))\r\n",
    "            x_list = ['x_'+str(crossline) for crossline in x_list]\r\n",
    "        else:\r\n",
    "            x_list = []\r\n",
    "\r\n",
    "        list_test = i_list + x_list\r\n",
    "\r\n",
    "        file_object = open(\r\n",
    "            pjoin((path_drive + 'data'), 'splits', 'section_' + split + '.txt'), 'w')\r\n",
    "        file_object.write('\\n'.join(list_test))\r\n",
    "        file_object.close()\r\n",
    "\r\n",
    "        # object to train\r\n",
    "        test_set = section_loader(is_transform=True,\r\n",
    "                               split=split,)\r\n",
    "\r\n",
    "        #construção do vetor de imagens de treino\r\n",
    "        test_samples, test_labels, weights_class = buildingSet(test_set)\r\n",
    "\r\n",
    "        test_.append(test_samples)\r\n",
    "        label_.append(test_labels)\r\n",
    "\r\n",
    "    return np.array(test_), np.array(label_)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Cria o dataset de teste\r\n",
    "arguments = {\r\n",
    "                  \"split\": 'both',\r\n",
    "                  \"crossline\": True,\r\n",
    "                  \"inline\": True,\r\n",
    "                }\r\n",
    "\r\n",
    "\"\"\"1. Dataset: building Set \"\"\"  \r\n",
    "teste_set, label_set = datasetTest(arguments)\r\n",
    "\r\n",
    "\"\"\"2. Dataset: tf dataset \"\"\"\r\n",
    "#teste 1\r\n",
    "test1_dataset = tfDataset(np.array(teste_set[0]), np.array(label_set[0]), num_classes, SHUFFLE_BUFFER_SIZE, BATCH_SIZE)\r\n",
    "\r\n",
    "#teste 2\r\n",
    "test2_dataset = tfDataset(np.array(teste_set[1]), np.array(label_set[1]), num_classes, SHUFFLE_BUFFER_SIZE, BATCH_SIZE)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Visualizar informações dos dados do teste 1\r\n",
    "test1_set = np.array(teste_set[0])\r\n",
    "test1_labels = np.array(label_set[0])\r\n",
    "print(\"Dados do teste 1\")\r\n",
    "print(\"Samples e labels: \", test1_set.shape, test1_labels.shape)\r\n",
    "\r\n",
    "fig, ax = plt.subplots(1,2)\r\n",
    "ax[0].imshow(test1_set[0][:,:,0])\r\n",
    "ax[1].imshow(test1_labels[0][:,:,0])\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Visualizar informações dos dados do teste 2\r\n",
    "test2_set = np.array(teste_set[1])\r\n",
    "test2_labels = np.array(label_set[1])\r\n",
    "print(\"Dados do teste 2\")\r\n",
    "print(\"Samples e labels: \", test2_set.shape, test2_labels.shape)\r\n",
    "\r\n",
    "fig, ax = plt.subplots(1,2)\r\n",
    "ax[0].imshow(test2_set[0][:,:,0])\r\n",
    "ax[1].imshow(test2_labels[0][:,:,0])\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "del test1_set, test1_labels, test2_set, test2_labels"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# dataset tensorflow\r\n",
    "print(\"Teste1 dataset tensorflow\", test1_dataset)\r\n",
    "print(\"Teste2 dataset tensorflow\", test2_dataset)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\"\"\"3. Test the model \"\"\"\r\n",
    "# Test 1\r\n",
    "loss, accuracy, precision, recall = model.evaluate(test1_dataset)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Test 2\r\n",
    "loss2, accuracy2, precision2, recall2 = model.evaluate(test2_dataset)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print('*** Resultado do teste 1 ***')\r\n",
    "print(f'ACURACIA: {accuracy}')\r\n",
    "print(f'PRECISION: {precision}')\r\n",
    "print(f'RECALL: {recall}')\r\n",
    "print(f'F1:{2 * ((precision*recall) / (precision + recall))}')\r\n",
    "\r\n",
    "print('*** Resultado do teste 2 ***')\r\n",
    "print(f'ACURACIA: {accuracy2}')\r\n",
    "print(f'PRECISION: {precision2}')\r\n",
    "print(f'RECALL: {recall2}')\r\n",
    "print(f'F1:{2 * ((precision2*recall2) / (precision2 + recall2))}')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\"\"\"4. Predição do teste \"\"\"\r\n",
    "# Teste 1\r\n",
    "prediction1 = model.predict(test1_dataset)\r\n",
    "print('Teste 1', prediction1.shape)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# deletar dados que não serão mais usados: conjuntos dos dados de teste 1\r\n",
    "del test1_dataset"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\"\"\"5. Visualização da predição do teste\"\"\"\r\n",
    "\r\n",
    "# *** TESTE 1 ***\r\n",
    "test1_set = np.array(teste_set[0])\r\n",
    "test1_labels = np.array(label_set[0])\r\n",
    "\r\n",
    "for i in range(300):\r\n",
    "    prediction_reconstructed = tf.cast(tf.argmax(prediction1[i], axis=-1), dtype='float32')\r\n",
    "    print(i)\r\n",
    "    fig, ax = plt.subplots(1,3, sharex=True, sharey=True)\r\n",
    "    ax[0].imshow(np.array(test1_set[i][:,:,0]))\r\n",
    "    ax[1].imshow(np.array(test1_labels[i][:,:,0]))\r\n",
    "    ax[2].imshow(prediction_reconstructed)\r\n",
    "    plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Teste 2\r\n",
    "prediction2 = model.predict(test2_dataset)\r\n",
    "print('Teste 2', prediction2.shape)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# deletar dados que não serão mais usados: conjuntos dos dados de teste \r\n",
    "del test2_dataset, test1_set, test1_labels"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# *** TESTE 2 ***\r\n",
    "test2_set = np.array(teste_set[1])\r\n",
    "test2_labels = np.array(label_set[1])\r\n",
    "for i in range(np.array(prediction2.shape[0])):\r\n",
    "    prediction_reconstructed = tf.cast(tf.argmax(prediction2[i], axis=-1), dtype='float32')\r\n",
    "    print(i)\r\n",
    "    fig, ax = plt.subplots(1,3, sharex=True, sharey=True)\r\n",
    "    ax[0].imshow(np.array(test2_set[i][:,:,0]))\r\n",
    "    ax[1].imshow(np.array(test2_labels[i][:,:,0]))\r\n",
    "    ax[2].imshow(prediction_reconstructed)\r\n",
    "    plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# deletar dados que não serão mais usados: conjuntos dos dados de teste\r\n",
    "del teste_set, test2_set, test2_labels"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.11 64-bit ('mestrado': conda)"
  },
  "interpreter": {
   "hash": "447cc6f42bbc2aea3361993c39d6f6c3959bdf1889010beb1442d5334007d2b1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}